---
title: "Coursera Data Science Capstone"
subtitle: "Prediction Model"
author: "Samuel Bozzi Baco"
output:
  html_document:
    df_print: paged
---
<style>
body {
text-align: justify}
</style>

This document aims to describe the word prediction model creation. This model will further be implemented on a Shiny App. 

This model is based on work from Thiloshon Nagarajah (https://thiloshon.wordpress.com/2018/03/11/build-your-own-word-sentence-prediction-application-part-02/).

## 0 LOAD PACKAGES

```{r LOAD PACKAGES, message=F, warning=F}
library(tidyverse)
library(tidytext)
library(lexicon)
library(stringr)
library(textclean)
library(furrr)
library(stringi)
library(tidyr)
library(qdap)
library(SnowballC)
library(tm)
```

## 1 DATA DOWNLOAD AND READING

The files were downloaded using a link available at Task 0 part of Coursera web site.

```{r DATA DOWLOAD, eval=F, message=F, warning=F}

setwd("~/R/Learning/Coursera/Course_10_Capstone")

fileLink <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

# Download raw data

download.file(url = fileLink,
              destfile = "dataset/rawData.zip")

# Extract the file

unzip("dataset/rawData.zip",
      exdir = "dataset")

```

The analysis will be focused on the English language. The data from three sources (blogs, news, and twitter) will be loaded at a list, to be transformed in a Corpus.

```{r DATA IMPORT, message=FALSE, warning=FALSE, cache=T}

# Import data

setwd("~/R/Learning/Coursera/Course_10_Capstone")

rawData <- list(blog = readLines("dataset/final/en_US/en_US.blogs.txt", 
                                 warn = F,
                                 skipNul = T,
                                 encoding = "UTF-8"),
                news = readLines("dataset/final/en_US/en_US.news.txt",
                                 warn = F,
                                 skipNul = T,
                                 encoding = "UTF-8"),
                twitter = readLines(file("dataset/final/en_US/en_US.twitter.txt",
                                         open = "rb"),
                                    warn = F,
                                    skipNul = T,
                                    encoding = "UTF-8")
)

#closeAllConnections()

```

## 2 DATA SAMPLING AND CLANING

Three samples of raw data will be constructed and cleaned, considering 0.01%, 0.1%, 1%, and 10%. As tibble data structure does not accept unequal length, the samples will be constructed for each source than put together after.

```{r SAMPLING AND CLEANING, message=F, warning=F, cache=T}

sample_clean <- function(rawData, size = 0.1) {
  
  # rawData is a list of 1 or more documents. size is the percentage (size) of the sample.
        
  plan(multiprocess)
  
  rawData %>%
        future_map(function (z) {
          set.seed(123456)
          sample(z, size = (size*length(z)))}) %>%
        future_map(replace_contraction) %>%
        future_map(replace_hash) %>%
        future_map(replace_internet_slang) %>%
        future_map(replace_url) %>%
        future_map(replace_non_ascii) %>%
        future_map(replace_incomplete) %>%
        future_map(rm_emoticon, trim = T) %>%
        # Replace accented characters
        future_map(stri_trans_general, 
            id = "Latin-ASCII") %>%
        # remove consecutive duplicated words
        future_map(str_remove_all, pattern = "(\\b\\S+)(?:\\s+\\1\\b)+") %>%
        # remove all numbers
        future_map(str_remove_all, pattern = "\\d+") %>%
        # remove repeated words
        future_map(str_remove_all, pattern = "([[:alpha:]])\\1{2,}") %>%
        # remove underscores
        future_map(str_remove_all, pattern = "_") %>%
        # remove ´s after contraction is cleaned
        future_map(str_remove_all, pattern = "'s") %>%
        future_map(as_tibble) %>% 
        bind_rows(.id = "Source")
  
}

rawSample1 <- sample_clean(rawData, size = 0.01)

```

## 3 DATA TOKENIZE

To avoid profane words, a function with several dictionaries will be created.

```{r PROFANE WORDS}

# Create profane words tibble

profaneWords <- 
        list(alvarez = toString(profanity_alvarez),
             arr_bad = toString(profanity_arr_bad),
             banned = toString(profanity_banned),
             racist = toString(profanity_racist),
             zac = toString(profanity_zac_anger)) %>%
        as_tibble() %>%
        gather(key = "lexicon") %>%
        unnest_tokens(word, value) %>%
        distinct(word, .keep_all = T)

```

With the samples cleaned, the next step is to tokenize. For the development of the model, the 1% sample size will be used, leaving the 10% sample to the final model "fit". Also, after tokenization, the words will be stemmed and the term frequency will be calculated.

```{r TOKENIZE AND STEM, message=F, warning=F}

origWord <-
        rawSample1 %>%
        select(-Source) %>%
        unnest_tokens(word, value) %>%
        anti_join(profaneWords) %>%
        distinct()
  

unigram <-
        rawSample1 %>%
        select(-Source) %>%
        unnest_tokens(word, value) %>%
        anti_join(profaneWords) %>%
        #anti_join(stop_words) %>%
        mutate(word = wordStem(word, language = "english")) %>%
        count(word, sort = T, name = "n.uni") 

bigrams <-
        rawSample1 %>%
        select(-Source) %>%
        unnest_tokens(word, value, token = "ngrams", n = 2) %>%
        separate(word, c("word1", "word2"), sep = " ", remove = F) %>%
        #filter(!word1 %in% stop_words$word) %>%
        #filter(!word2 %in% stop_words$word) %>%
        filter(!word1 %in% profaneWords$word) %>%
        filter(!word2 %in% profaneWords$word) %>%
        mutate(word1 = wordStem(word1, language = "english"),
               word2 = wordStem(word2, language = "english")) %>%
        count(word1, word2, sort = TRUE, name = "n.bi")

trigrams <-
        rawSample1 %>%
        select(-Source) %>%
        unnest_tokens(word, value, token = "ngrams", n = 3) %>%
        separate(word, 
                c("word1", "word2", "word3"), 
                sep = " ", 
                remove = F) %>%
        #filter(!word1 %in% stop_words$word) %>%
        #filter(!word2 %in% stop_words$word) %>%
        #filter(!word3 %in% stop_words$word) %>%
        filter(!word1 %in% profaneWords$word) %>%
        filter(!word2 %in% profaneWords$word) %>%
        filter(!word3 %in% profaneWords$word) %>%
        mutate(word1 = wordStem(word1, language = "english"),
               word2 = wordStem(word2, language = "english"),
               word3 = wordStem(word3, language = "english")) %>%
        count(word1, word2, word3, sort = TRUE, name = "n.tri")

```

## 4 KNESER-NEY SMOTHING

With the tokens, it is possible to apply the Kneser-Ney smother, so the probabilities for each word, depending on the history, is estimated.

```{r KNESER-NEY, message=F, warning=F}

discValue <- 0.75

# Count word that occurs as second word in a bi-gram

ckn <-
  bigrams %>%
  count(word2, sort = TRUE) %>%
  mutate(uniProb = n/sum(n)) %>%
  select(-n) %>%
  rename("word" = "word2")

# Assign probabilities as second word of bi-gram to uni-grams

unigram <-
  unigram %>%
  left_join(ckn) %>%
  filter(!is.na(uniProb))

# Count a word that occurs as first word at a bi-gram

n1wi <-
  bigrams %>%
  count(word1, sort = T, name = "N")

# Assigning total times word 1 occurred to bi-grams 

bigrams <-
  bigrams %>%
  left_join(unigram, by = c("word1" = "word")) %>%
  rename("Cn1" = "n.uni") %>%
  select(-uniProb)

# Kneser Ney Smothing Algorithm for bi-Grams

biKn <-
  bigrams %>%
  left_join(n1wi) %>%
  left_join(unigram, by = c("word2" = "word")) %>%
  mutate(biProb = (n.bi - discValue) / Cn1 + discValue / Cn1 * N * uniProb)

# Finding count of word1-word2 combination in bi-grams

trigrams <-
  trigrams %>%
  left_join(select(bigrams, word1, word2, n.bi), by = c("word1", "word2")) %>%
  rename("Cn2" = "n.bi")

# Finding count of word1-word2 combination in tri-grams

n1w12 <-
  trigrams %>%
  count(word1, word2, sort = T, name = "N")

#Kneser Ney Smothing for Tri-grams

triKn <-
  trigrams %>%
  left_join(n1w12) %>%
  left_join(select(biKn, biProb, word1, word2), by = c("word1", "word2")) %>%
  mutate(triProb = (n.tri - discValue) / Cn2 + discValue / Cn2 * N * biProb)

```

## 5 PREDICTION APP

The first function is to predict the third word, given the previous two.

```{r PREDICT TRIGRAMS}

tri_fun <- function(w1, w2, n = 5){
        
        require(dplyr)
        require(tm)
        
        pwords <-
                triKn %>%
                filter(word1 == as.character(w1)) %>%
                filter(word2 == as.character(w2))
        
        if (dim(pwords)[1] == 0){
                
                # backoff to bi-grams prediction
                
                return(bi_fun(w2, n))
                #return(print("bi_fun"))
        }
        
        if (nrow(pwords) > n){
                
                # Return only words from tri-gram
                
                p <- 
                        pwords %>% 
                        arrange(-triProb) %>%
                        select(word3) %>%
                        filter(row_number() <= n)
                
                return(unname(stemCompletion(p$word3, 
                                             dictionary = origWord$word, 
                                             type = "prevalent")))
                                
        }
        
        count <- nrow(pwords)
        
        bwords <- bi_fun(w2, (n - count))
        
        return(unname(stemCompletion(c(p$word3, bwords), 
                                     dictionary = origWord$word, 
                                     type = "prevalent")))
        
}

```

If the function doesn’t find a tri-gram with the two given words, we back off to the bi-gram. We find the next word given one previous word. The backoff algorithm is linked inside each prediction functions

```{r PREDICT BIGRAMS}

bi_fun <- function(w2, n){
        
        pwords <-
                biKn %>%
                filter(word1 == as.character(w2))
        
        if(dim(pwords)[1] == 0){
                
                # Return a word weight by its probability from uni-grams
                
                return(uni_fun(n))
                
        } 
        
        if(nrow(pwords) > n){
                
                # Return only words from bi-gram
                
                p <-
                        pwords %>% 
                        arrange(-biProb) %>%
                        select(word2) %>%               
                        filter(row_number() <= n)
                
                return(unname(stemCompletion(p$word2, 
                                             dictionary = origWord$word, 
                                             type = "prevalent")))
                
        }
        
        # If the number of words is smaller than n, the function return all 
        # from bi-gram than completes from uni-gram data.
        
        count <- nrow(pwords)
        
        unWords <- uni_fun((n - count))
        
        return(unname(stemCompletion(c(pwords$word2, unWords), 
                                     dictionary = origWord$word, 
                                     type = "prevalent")))
        
}

```

If we could not even find the corresponding word from bi-gram, we randomly get an "n" words from uni-gram. The sample is made considering the word probability. This is the last resort for n-grams that are not found in the dataset.

```{r PREDICT UNIGRAM}

uni_fun <- function(size){
        
        # The function return "size" words, weighted by the probability of 
        # happening.
        
        return(unname(stemCompletion(sample(unigram$word, 
                                            size = size,
                                            prob = unigram$uniProb), 
                                     dictionary = origWord$word, 
                                     type = "prevalent")))
        
}

```

Finally, a function to bind all others.

```{r APP}

getWords <- function(str){
    
    require(tidytext)
    require(dplyr)
    require(SnowballC)
    
    tokens <-
          str %>%
          as_tibble() %>%
          bind_rows() %>%
          unnest_tokens(word, value) %>%
          mutate(word = wordStem(word, language = "english")) %>%
          arrange(-row_number()) %>%
          filter(row_number() <= 2) %>%
          arrange(-row_number())
    
    if(dim(tokens)[1] < 2){
      
      words <- bi_fun(tokens[1,1], 5)
      
      return(print(words))
      
    }else{
      
      words <- tri_fun(tokens[1,1], tokens[2,1], 5)
    
      return(paste(words, sep = ", "))
      
    }
    
}

```

Lets test some words:

```{r MODEL TEST}

getWords("Tomorrow")

```
```{r MODEL TEST2}

getWords("I love")

```
```{r MODEL TEST3}

getWords("Shall we go to")

```











