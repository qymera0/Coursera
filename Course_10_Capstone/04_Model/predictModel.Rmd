---
title: "Coursera Data Science word Prediction Model"
output: html_notebook
---
<style>
body {
text-align: justify}
</style>

This document aims to describe the word prediction model creation. This model will futher be implemented on a Shiny App.

## 0 LOAD PACKAGES

```{r LOAD PACKAGES, message=F, warning=F}
library(tidyverse)
library(tidytext)
library(lexicon)
library(stringr)
library(textclean)
library(furrr)
library(stringi)
library(tidyr)
library(qdap)
library(SnowballC)
```

## 1 DATA DOWNLOAD AND READING

The files were downloaded using a link available at Task 0 part of Coursera web site.

```{r DATA DOWLOAD, eval=F, message=F, warning=F}

fileLink <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

# Download raw data

download.file(url = fileLink,
              destfile = "dataset/rawData.zip")

# Extract the file

unzip("dataset/rawData.zip",
      exdir = "dataset")

```

The analysis will be focused on English language. The data from three sources (blogs, news and twitter) will be loaded at a list, to be transformed in a Corpus.

```{r DATA IMPORT, message=FALSE, warning=FALSE}

# Import data

setwd("~/Google Drive/Data Science/Learning/Coursera/Course_10_Capstone")

rawData <- list(blog = readLines("dataset/final/en_US/en_US.blogs.txt", 
                                 warn = F,
                                 skipNul = T,
                                 encoding = "UTF-8"),
                news = readLines("dataset/final/en_US/en_US.news.txt",
                                 warn = F,
                                 skipNul = T,
                                 encoding = "UTF-8"),
                twitter = readLines(file("dataset/final/en_US/en_US.twitter.txt",
                                         open = "rb"),
                                    warn = F,
                                    skipNul = T,
                                    encoding = "UTF-8")
)

closeAllConnections()

```

## 2 DATA SAMPLING AND CLANING

Three samples of raw data will be constructed and cleaned, considering 0.01%, 0.1%, 1% and 10%. As tibble data structure does not accept unequal length, the samples will be constructed for each source than put together after.

```{r SAMPLING AND CLEANING}

sample_clean <- function(rawData, size = 0.1) {
  
  # rawData is a list of 1 or more documents. size is the percentage (size) of the sample.
        
  plan(multiprocess)
  
  rawData %>%
        future_map(function (z) {
          set.seed(123456)
          sample(z, size = (size*length(z)))}) %>%
        future_map(replace_contraction) %>%
        future_map(replace_hash) %>%
        future_map(replace_internet_slang) %>%
        future_map(replace_url) %>%
        future_map(replace_non_ascii) %>%
        future_map(replace_incomplete) %>%
        future_map(rm_emoticon, trim = T) %>%
        # Replace accented characters
        future_map(stri_trans_general, 
            id = "Latin-ASCII") %>%
        # remove consecutive duplicated words
        future_map(str_remove_all, pattern = "(\\b\\S+)(?:\\s+\\1\\b)+") %>%
        # remove all numbers
        future_map(str_remove_all, pattern = "\\d+") %>%
        # remove repeated words
        future_map(str_remove_all, pattern = "([[:alpha:]])\\1{2,}") %>%
        # remove underscores
        future_map(str_remove_all, pattern = "_") %>%
        # remove Â´s after contraction is cleaned
        future_map(str_remove_all, pattern = "'s") %>%
        future_map(as_tibble) %>% 
        bind_rows(.id = "Source")
  
}

rawSample1 <- sample_clean(rawData, size = 0.01)

```

## 3 DATA TOKENIZE

To avoid profane words, a function with several dictionaries will be created.

```{r PROFANE WORDS}

# Create profane words tibble

profaneWords <- 
        list(alvarez = toString(profanity_alvarez),
             arr_bad = toString(profanity_arr_bad),
             banned = toString(profanity_banned),
             racist = toString(profanity_racist),
             zac = toString(profanity_zac_anger)) %>%
        as_tibble() %>%
        gather(key = "lexicon") %>%
        unnest_tokens(word, value) %>%
        distinct(word, .keep_all = T)

```

With the samples cleaned, the next step is to tokenize. For the development of the model, the 1% sample size will be used, leaving the 10% sample to final model "fit". Also, after tokenization, the words will be stemmed and the term frequency will be calculated.

```{r TOKENIZE AND STEM, message=F, warning=F}

unigram <-
        rawSample1 %>%
        select(-Source) %>%
        unnest_tokens(word, value) %>%
        anti_join(profaneWords) %>%
        anti_join(stop_words) %>%
        mutate(word = wordStem(word, language = "english")) %>%
        count(word, sort = T, name = "n.uni") 

bigrams <-
        rawSample1 %>%
        select(-Source) %>%
        unnest_tokens(word, value, token = "ngrams", n = 2) %>%
        separate(word, c("word1", "word2"), sep = " ", remove = F) %>%
        filter(!word1 %in% stop_words$word) %>%
        filter(!word2 %in% stop_words$word) %>%
        filter(!word1 %in% profaneWords$word) %>%
        filter(!word2 %in% profaneWords$word) %>%
        mutate(word1 = wordStem(word1, language = "english"),
               word2 = wordStem(word2, language = "english")) %>%
        count(word1, word2, sort = TRUE, name = "n.bi")

trigrams <-
        rawSample1 %>%
        select(-Source) %>%
        unnest_tokens(word, value, token = "ngrams", n = 3) %>%
        separate(word, 
                c("word1", "word2", "word3"), 
                sep = " ", 
                remove = F) %>%
        filter(!word1 %in% stop_words$word) %>%
        filter(!word2 %in% stop_words$word) %>%
        filter(!word3 %in% stop_words$word) %>%
        filter(!word1 %in% profaneWords$word) %>%
        filter(!word2 %in% profaneWords$word) %>%
        filter(!word3 %in% profaneWords$word) %>%
        mutate(word1 = wordStem(word1, language = "english"),
               word2 = wordStem(word2, language = "english"),
               word3 = wordStem(word3, language = "english")) %>%
        count(word1, word2, word3, sort = TRUE, name = "n.tri")

```

With the tokens, it is possible to apply the Kneser-Ney smother, so the probabilities for each word, depending of the history, is estimated.

```{r KNESER-NEY, message=F, warning=F}

discValue <- 0.75

# Count word that occurs as second word in a bi-gram

ckn <-
  bigrams %>%
  count(word2, sort = TRUE) %>%
  mutate(uniProb = n/sum(n)) %>%
  select(-n) %>%
  rename("word" = "word2")

# Assign probabilities as second word of bi-gram to uni-grams

unigram <-
  unigram %>%
  left_join(ckn) %>%
  filter(!is.na(uniProb))

# Count a word that occurs as first word at a bi-gram

n1wi <-
  bigrams %>%
  count(word1, sort = T, name = "N")

# Assigning total times word 1 occurred to bi-grams 

bigrams <-
  bigrams %>%
  left_join(unigram, by = c("word1" = "word")) %>%
  rename("Cn1" = "n.uni") %>%
  select(-uniProb)

# Kneser Ney Smothing Algorithm for bi-Grams

biKn <-
  bigrams %>%
  left_join(n1wi) %>%
  left_join(unigram, by = c("word2" = "word")) %>%
  mutate(biProb = (n.bi - discValue) / Cn1 + discValue / Cn1 * N * uniProb)

# Finding count of word1-word2 combination in bi-grams

trigrams <-
  trigrams %>%
  left_join(select(bigrams, word1, word2, n.bi), by = c("word1", "word2")) %>%
  rename("Cn2" = "n.bi")

# Finding count of word1-word2 combination in tri-grams

n1w12 <-
  trigrams %>%
  count(word1, word2, sort = T, name = "N")

#Kneser Ney Smothing for Tri-grams

triKn <-
  trigrams %>%
  left_join(n1w12) %>%
  left_join(select(biKn, biProb, word1, word2), by = c("word1", "word2")) %>%
  mutate(Prob = (n.tri - discValue) / Cn2 + discValue / Cn2 * N * biProb)

```



