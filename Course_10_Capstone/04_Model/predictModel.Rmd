---
title: "Coursera Data Science word Prediction Model"
output: html_notebook
---
<style>
body {
text-align: justify}
</style>

This document aims to describe the word prediction model creation. This model will futher be implemented on a Shiny App. 

This model is based on work from Thiloshon Nagarajah (https://thiloshon.wordpress.com/2018/03/11/build-your-own-word-sentence-prediction-application-part-02/).

## 0 LOAD PACKAGES

```{r LOAD PACKAGES, message=F, warning=F}
library(tidyverse)
library(tidytext)
library(lexicon)
library(stringr)
library(textclean)
library(furrr)
library(stringi)
library(tidyr)
library(qdap)
library(SnowballC)
```

## 1 DATA DOWNLOAD AND READING

The files were downloaded using a link available at Task 0 part of Coursera web site.

```{r DATA DOWLOAD, eval=F, message=F, warning=F}

fileLink <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

# Download raw data

download.file(url = fileLink,
              destfile = "dataset/rawData.zip")

# Extract the file

unzip("dataset/rawData.zip",
      exdir = "dataset")

```

The analysis will be focused on English language. The data from three sources (blogs, news and twitter) will be loaded at a list, to be transformed in a Corpus.

```{r DATA IMPORT, message=FALSE, warning=FALSE}

# Import data

setwd("~/R/Learning/Coursera/Course_10_Capstone")

rawData <- list(blog = readLines("dataset/final/en_US/en_US.blogs.txt", 
                                 warn = F,
                                 skipNul = T,
                                 encoding = "UTF-8"),
                news = readLines("dataset/final/en_US/en_US.news.txt",
                                 warn = F,
                                 skipNul = T,
                                 encoding = "UTF-8"),
                twitter = readLines(file("dataset/final/en_US/en_US.twitter.txt",
                                         open = "rb"),
                                    warn = F,
                                    skipNul = T,
                                    encoding = "UTF-8")
)

closeAllConnections()

```

## 2 DATA SAMPLING AND CLANING

Three samples of raw data will be constructed and cleaned, considering 0.01%, 0.1%, 1% and 10%. As tibble data structure does not accept unequal length, the samples will be constructed for each source than put together after.

```{r SAMPLING AND CLEANING}

sample_clean <- function(rawData, size = 0.1) {
  
  # rawData is a list of 1 or more documents. size is the percentage (size) of the sample.
        
  plan(multiprocess)
  
  rawData %>%
        future_map(function (z) {
          set.seed(123456)
          sample(z, size = (size*length(z)))}) %>%
        future_map(replace_contraction) %>%
        future_map(replace_hash) %>%
        future_map(replace_internet_slang) %>%
        future_map(replace_url) %>%
        future_map(replace_non_ascii) %>%
        future_map(replace_incomplete) %>%
        future_map(rm_emoticon, trim = T) %>%
        # Replace accented characters
        future_map(stri_trans_general, 
            id = "Latin-ASCII") %>%
        # remove consecutive duplicated words
        future_map(str_remove_all, pattern = "(\\b\\S+)(?:\\s+\\1\\b)+") %>%
        # remove all numbers
        future_map(str_remove_all, pattern = "\\d+") %>%
        # remove repeated words
        future_map(str_remove_all, pattern = "([[:alpha:]])\\1{2,}") %>%
        # remove underscores
        future_map(str_remove_all, pattern = "_") %>%
        # remove ´s after contraction is cleaned
        future_map(str_remove_all, pattern = "'s") %>%
        future_map(as_tibble) %>% 
        bind_rows(.id = "Source")
  
}

rawSample1 <- sample_clean(rawData, size = 0.01)

```

## 3 DATA TOKENIZE

To avoid profane words, a function with several dictionaries will be created.

```{r PROFANE WORDS}

# Create profane words tibble

profaneWords <- 
        list(alvarez = toString(profanity_alvarez),
             arr_bad = toString(profanity_arr_bad),
             banned = toString(profanity_banned),
             racist = toString(profanity_racist),
             zac = toString(profanity_zac_anger)) %>%
        as_tibble() %>%
        gather(key = "lexicon") %>%
        unnest_tokens(word, value) %>%
        distinct(word, .keep_all = T)

```

With the samples cleaned, the next step is to tokenize. For the development of the model, the 1% sample size will be used, leaving the 10% sample to final model "fit". Also, after tokenization, the words will be stemmed and the term frequency will be calculated.

```{r TOKENIZE AND STEM, message=F, warning=F}

unigram <-
        rawSample1 %>%
        select(-Source) %>%
        unnest_tokens(word, value) %>%
        anti_join(profaneWords) %>%
        #anti_join(stop_words) %>%
        mutate(word = wordStem(word, language = "english")) %>%
        count(word, sort = T, name = "n.uni") 

bigrams <-
        rawSample1 %>%
        select(-Source) %>%
        unnest_tokens(word, value, token = "ngrams", n = 2) %>%
        separate(word, c("word1", "word2"), sep = " ", remove = F) %>%
        #filter(!word1 %in% stop_words$word) %>%
        #filter(!word2 %in% stop_words$word) %>%
        filter(!word1 %in% profaneWords$word) %>%
        filter(!word2 %in% profaneWords$word) %>%
        mutate(word1 = wordStem(word1, language = "english"),
               word2 = wordStem(word2, language = "english")) %>%
        count(word1, word2, sort = TRUE, name = "n.bi")

trigrams <-
        rawSample1 %>%
        select(-Source) %>%
        unnest_tokens(word, value, token = "ngrams", n = 3) %>%
        separate(word, 
                c("word1", "word2", "word3"), 
                sep = " ", 
                remove = F) %>%
        #filter(!word1 %in% stop_words$word) %>%
        #filter(!word2 %in% stop_words$word) %>%
        #filter(!word3 %in% stop_words$word) %>%
        filter(!word1 %in% profaneWords$word) %>%
        filter(!word2 %in% profaneWords$word) %>%
        filter(!word3 %in% profaneWords$word) %>%
        mutate(word1 = wordStem(word1, language = "english"),
               word2 = wordStem(word2, language = "english"),
               word3 = wordStem(word3, language = "english")) %>%
        count(word1, word2, word3, sort = TRUE, name = "n.tri")

```

## 4 KNESER-NEY SMOTHING

With the tokens, it is possible to apply the Kneser-Ney smother, so the probabilities for each word, depending of the history, is estimated.

```{r KNESER-NEY, message=F, warning=F}

discValue <- 0.75

# Count word that occurs as second word in a bi-gram

ckn <-
  bigrams %>%
  count(word2, sort = TRUE) %>%
  mutate(uniProb = n/sum(n)) %>%
  select(-n) %>%
  rename("word" = "word2")

# Assign probabilities as second word of bi-gram to uni-grams

unigram <-
  unigram %>%
  left_join(ckn) %>%
  filter(!is.na(uniProb))

# Count a word that occurs as first word at a bi-gram

n1wi <-
  bigrams %>%
  count(word1, sort = T, name = "N")

# Assigning total times word 1 occurred to bi-grams 

bigrams <-
  bigrams %>%
  left_join(unigram, by = c("word1" = "word")) %>%
  rename("Cn1" = "n.uni") %>%
  select(-uniProb)

# Kneser Ney Smothing Algorithm for bi-Grams

biKn <-
  bigrams %>%
  left_join(n1wi) %>%
  left_join(unigram, by = c("word2" = "word")) %>%
  mutate(biProb = (n.bi - discValue) / Cn1 + discValue / Cn1 * N * uniProb)

# Finding count of word1-word2 combination in bi-grams

trigrams <-
  trigrams %>%
  left_join(select(bigrams, word1, word2, n.bi), by = c("word1", "word2")) %>%
  rename("Cn2" = "n.bi")

# Finding count of word1-word2 combination in tri-grams

n1w12 <-
  trigrams %>%
  count(word1, word2, sort = T, name = "N")

#Kneser Ney Smothing for Tri-grams

triKn <-
  trigrams %>%
  left_join(n1w12) %>%
  left_join(select(biKn, biProb, word1, word2), by = c("word1", "word2")) %>%
  mutate(triProb = (n.tri - discValue) / Cn2 + discValue / Cn2 * N * biProb)

```

## 5 PREDICTION APP

The first function is to predict the third word, given previous two.

```{r PREDICT TRIGRAMS}

tri_fun <- function(w1, w2, n = 5){
  
  require(dplyr)
  
  pwords <-
    triKn %>%
    filter(word1 == as.character(w1)) %>%
    filter(word2 == as.character(w2))
  
  if (dim(pwords)[1] == 0)
    
    # backoff to bi-grams prediction
    
    return(bi_fun(w2, n)) 
  
  if (nrow(pwords) > n)
    
    return(as.character(pwords %>% 
                             arrange(-biProb) %>% 
                             filter(row_number() <= n)$word2)) # propose 5 options
           
  count <- nrow(pwords)
  
  bwords <- 
    bi_fun(w2, n) %>%
    top(n - count)
  
  return(c(pwords[ , word3], bwords))
  
}

```

If we don’t find a tri-gram with the two given words, we backoff to the bi-gram. We find the next word given one previous word.

The backoff algorithm is linked inside each prediction functions

```{r PREDICT BIGRAMS}

bi_fun <- function(w1, n = 5){
  
  require(dplyr)
  
  pwords <-
      biKn %>%
      filter(word1 == as.character(w1))

  if(dim(pwords)[1] == 0){
        
        # Return a word weight by its probability from uni-grams
      
        return(uni_fun())
        
  } 
    
  if(nrow(pwords) > n){
      
      # Return only words from bi-gram
        
       return(as.character(pwords %>% 
                           arrange(-biProb) %>% 
                           filter(row_number() <= n)$word2))
        
  }

  # If the number of words is smaller than n, the function return all 
  # from bi-gram than completes from uni-gram data.
    
  count <- nrow(pwords)

  unWords <- uni_fun()[1:n-count]
    
  return(c(pwords$word2, unWords))
    
}

```

If we could not even find the corresponding word from bi-gram, we randomly get a "n" words from uni-gram. The sample is made considering the word probability. This is the last resort for n-grams that are not found in the dataset.

```{r PREDICT UNIGRAM}

uni_fun <- function(n = 5){
        
        # The function return "size" words, weighted by the probability of 
        # happening.
        
        return(sample(unigram$word, size = n, prob = unigram$uniProb))
        
}

```

Finally, a function to bind all these.

```{r APP}

getWords <- function(str){
    
    require(tidytext)
    require(dplyr)
    require(SnowballC)
    
    tokens <-
        str %>%
        as.tibble() %>%
        bind_rows() %>%
        unnest_tokens(word, value) %>%
        mutate(word = wordStem(word, language = "english"))
      
    
    words <- tri_fun(tokens[1,1], tokens[2,1], 5)
    
    chain1 <- paste(tokens[1,1], tokens[2,1], words[1], sep = " ")

    print(words[1])
}

```












