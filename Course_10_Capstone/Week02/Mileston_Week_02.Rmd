---
title: "Coursera HJ Data Science Capstone Week 02"
author: "Samuel Bozzi Baco"
output: html_notebook
---

<style>
body {
text-align: justify}
</style>

## LOAD PACKAGES

```{r LOAD PACKAGES, message=F, warning=F}
library(tidyverse)
library(tidytext)
library(lexicon)
library(stringr)
library(textclean)
library(furrr)
library(stringi)
```


## INTRODUCTION

This document presents the code and analysis for tasks 1 (get and cleaning data), 2 (exploratory data analysis), and 3 (preliminary model), all of them related to the capstone project from John´s Hopkins data science program.

The project is related to build a key that takes a (or more than 1)  word written by the user and (using ML prediction) suggest the next key.

Differently from suggested, instead of *tm* package, the *tidytext* approach will be used, mainly based on Julia Silge and David Robinson book “Text Mining with R: a Tidy approach”, delivered by Bookdown site (https://bookdown.org/)

## TASK 01: GETTING AND CLEANING THE DATA

### 1.1 Data download

The files were downloaded using a link available at Task 0 part of Coursera web site.

```{r DATA DOWLOAD, eval=F, message=F, warning=F}

fileLink <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

# Download raw data

download.file(url = fileLink,
              destfile = "dataset/rawData.zip")

# Extract the file

unzip("dataset/rawData.zip",
      exdir = "dataset")

```

### 1.2 Data import and cleaning

The initial analysis will be focused on English language. The data from three sources (blogs, news and twitter) will be loaded at a list, to be transformed in a Corpus.

```{r DATA IMPORT, message=F, warning=F}

# Import data

rawData <- list(blog = readLines("dataset/final/en_US/en_US.blogs.txt", 
                                 warn = F,
                                 skipNul = T,
                                 encoding = "UTF-8"),
                news = readLines("dataset/final/en_US/en_US.news.txt",
                                 warn = F,
                                 skipNul = T,
                                 encoding = "UTF-8"),
                twitter = readLines(file("dataset/final/en_US/en_US.twitter.txt",
                                         open = "rb"),
                                    warn = F,
                                    skipNul = T,
                                    encoding = "UTF-8")
)

closeAllConnections()

```

As blog and twitter text data are usually little messy (with a lot of misspellimg words, emoticons, contractions), the package *textclean* will be used to improve the readability of the text. In addition, some specific patterns like numbers, consecutive repeated words and repeteated characters (word elongation) will be removed from text.

```{r SAMPLE AND CLEAN, message = F, warning = F}

# Sample and clean

rawSample <-
        rawData %>%
        map(function (x) {
          set.seed(123456)
          sample(x, 20000)}) %>%
        map(replace_contraction) %>%
        map(replace_emoji) %>%
        map(replace_hash) %>%
        map(replace_internet_slang) %>%
        map(replace_url) %>%
        map(replace_non_ascii) %>%
        map(replace_incomplete) %>%
        # Replace accented characters
        map(stri_trans_general, 
            id = "Latin-ASCII") %>%
        # remove consecutive duplicated words
        map(str_remove_all, pattern = "(\\b\\S+)(?:\\s+\\1\\b)+") %>%
        # remove all numbers
        map(str_remove_all, pattern = "\\d+") %>%
        # remove repeated words
        map(str_remove_all, pattern = "([[:alpha:]])\\1{2,}") %>%
        as_tibble() %>%
        gather(key = "source") %>%
        mutate(source = as.factor(source))

```

Raw data was sample to allow more velocity on posterior EDA and preliminary modeling and was tidified putting all data on a tibble.

In addition, the dataframe with profane words will be constructed, using all sources from *lexicon* package.

```{r PROFANE WORDS}

# Create profane words tibble

profaneWords <- 
        list(alvarez = toString(profanity_alvarez),
             arr_bad = toString(profanity_arr_bad),
             banned = toString(profanity_banned),
             racist = toString(profanity_racist),
             zac = toString(profanity_zac_anger)) %>%
        as_tibble() %>%
        gather(key = "lexicon") %>%
        unnest_tokens(word, value) %>%
        distinct(word, .keep_all = T)

```

### 1.3 Tokenize

The cleaning process will have this steps:

* Tokenize sources,
* remove profane words,
* remove stop words,

```{r DATA CLEANING, message=F, warning=T}

# Tokenize data

unigram <-
        rawSample %>%
        unnest_tokens(word, value) %>%
        anti_join(profaneWords) %>%
        anti_join(stop_words) %>%
        count(word, sort = T) %>%
        mutate(total = sum(n),
               tf = n/sum(n))

```

The same process to tokenize and cleaning will be used to bi-grams, tri-grams and quad-grams.

```{r N-GRAMS, message=F}

# bi-grams

bigrams <-
  rawSample %>%
  unnest_tokens(word, value, token = "ngrams", n = 2) %>%
  count(word, sort = TRUE) %>%
  separate(word, c("word1", "word2"), sep = " ", remove = F) %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word1 %in% profaneWords$word) %>%
  filter(!word2 %in% profaneWords$word) %>%
  mutate(total = sum(n),
         tf = n/sum(n))

# tri-gram

trigrams <-
  rawSample %>%
  unnest_tokens(word, value, token = "ngrams", n = 3) %>%
  count(word, sort = TRUE) %>%
  separate(word, 
           c("word1", "word2", "word3"), 
           sep = " ", 
           remove = F) %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>%
  filter(!word1 %in% profaneWords$word) %>%
  filter(!word2 %in% profaneWords$word) %>%
  filter(!word3 %in% profaneWords$word) %>%
  mutate(total = sum(n),
         tf = n/sum(n))

# quad-gram

quadgrams <-
  rawSample %>%
  unnest_tokens(word, value, token = "ngrams", n = 4) %>%
  count(word, sort = TRUE) %>%
  separate(word, 
           c("word1", "word2", "word3", "word4"), 
           sep = " ", 
           remove = F) %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word) %>%
  filter(!word4 %in% stop_words$word) %>%
  filter(!word1 %in% profaneWords$word) %>%
  filter(!word2 %in% profaneWords$word) %>%
  filter(!word3 %in% profaneWords$word) %>%
  filter(!word4 %in% profaneWords$word) %>%
  mutate(total = sum(n),
         tf = n/sum(n))

```

## TASK 02: EXPLORATORY DATA ANALYSIS

### 2.1 Ploting n-grams frequency

#### Uni-grams

```{r UNIGRAMS PLOT}

unigram %>%
  arrange(desc(n)) %>%
  group_by(source) %>%
  top_n(15) %>%
  mutate(word = factor(word, levels = rev(unique(word))))
  ungroup() %>%
  ggplot(aes(word, n, fill = source)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, 
       y = "Word count",
       man = "Unigram frequency") + 
  facet_wrap(~ source, ncol = 3, scales = "free") + 
  coord_flip()

```



## TASK 03: PRELIMINAR MODELING