---
title: "Coursera HJ Data Science Capstone Week 02"
author: "Samuel Bozzi Baco"
output: html_notebook
---

<style>
body {
text-align: justify}
</style>

## LOAD PACKAGES

```{r LOAD PACKAGES, message=F, warning=F}
library(tidyverse)
library(tidytext)
library(tm)
library(lexicon)
library(data.table)
library(stringr)
library(stringi)
```


## INTRODUCTION

This document presents the code and analysis for tasks 1 (get and cleaning data), 2 (exploratory data analysis), and 3 (preliminary model), all of them related to the capstone project from John´s Hopkins data science program.

The project is related to build a key that takes a (or more than 1)  word written by the user and (using ML prediction) suggest the next key.

Differently from suggested, instead of *tm* package, the *tidytext* approach will be used, mainly based on Julia Silge and David Robinson book “Text Mining with R: a Tidy approach”, delivered by Bookdown site (https://bookdown.org/)

## TASK 01: GETTING AND CLEANING THE DATA

### 1.1 Data download

The files were downloaded using a link available at Task 0 part of Coursera web site.

```{r DATA DOWLOAD, eval=F, message=F, warning=F}

fileLink <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

# Download raw data

download.file(url = fileLink,
              destfile = "dataset/rawData.zip")

# Extract the file

unzip("dataset/rawData.zip",
      exdir = "dataset")

# Download profanity data 

download.file("http://www.cs.cmu.edu/~biglou/resources/bad-words.txt", 
              destfile = "dataset/bad-words.txt", method = "curl")

```

### 1.2 Data import

The initial analysis will be focused on English language. The data from three sources (blogs, news and twitter) will be loaded at a list, to be transformed in a Corpus.

In addition, the dataframe with profane words will be constructed, usign all sources from *lexicon* package.

```{r DATA IMPORT}

# Import data

rawData <- list(blog = readLines("dataset/final/en_US/en_US.blogs.txt", 
                                 warn = F,
                                 skipNul = T),
                news = readLines("dataset/final/en_US/en_US.news.txt",
                                 warn = F,
                                 skipNul = T),
                twitter = readLines(file("dataset/final/en_US/en_US.twitter.txt",
                                         open = "rb"),
                                    warn = F,
                                    skipNul = T)
)

# Create profane words tibble

profaneWords <- 
        list(alvarez = toString(profanity_alvarez),
             arr_bad = toString(profanity_arr_bad),
             banned = toString(profanity_banned),
             racist = toString(profanity_racist),
             zac = toString(profanity_zac_anger)) %>%
        as_tibble() %>%
        gather(key = "lexicon") %>%
        unnest_tokens(word, value) %>%
        distinct(word, .keep_all = T)
       
# Sample and tidify data

rawSample <-
        rawData %>%
        map(sample, 20000) %>%
        as_tibble() %>%
        gather(key = "source") %>%
        mutate(source = as.factor(source))

```

Raw data was sample to allow more velocity on posterior EDA and preliminary modeling and was tidified putting all data on a tibble.

### 1.3 Data cleaning

The cleaning process will have this steps:

* Tokenize sources,
* remove profane words,
* remove stop words,
* remove words that start with a number,
* remove URL´s,
* remove the words that are just punctuation or just a number.

```{r DATA CLEANING, message=F, warning=T}

# Tokenize data

token <-
        rawSample %>%
        unnest_tokens(word, value) %>%
        anti_join(profaneWords) %>%
        anti_join(stop_words) %>%
        filter(!str_detect(word, "^\\d+"),
               !str_detect(word, "(f|ht)tp(s?)://(.*)[.][a-z]+"),
               !str_detect(word, "@[^\\s]+"),
               !stri_detect_regex(word,"[0-9]+|[[:punct:]]|\\(.*\\)")) %>%
        count(source, word, sort = T)

# Calculate total of words per source

totalWords <-
        token %>%
        group_by(source) %>%
        summarise(total = sum(n))

# Put together all info

token <- left_join(token, totalWords)

```



## TASK 02: EXPLORATORY DATA ANALYSIS

## TASK 03: PRELIMINAR MODELING