---
title: "Coursera HJ Data Science Capstone Week 02"
author: "Samuel Bozzi Baco"
output: html_notebook
---

<style>
body {
text-align: justify}
</style>

## LOAD PACKAGES

```{r LOAD PACKAGES, message=F, warning=F}
library(tidyverse)
library(tidytext)
library(lexicon)
library(stringr)
library(textclean)
library(furrr)
library(qdap)
```


## INTRODUCTION

This document presents the code and analysis for tasks 1 (get and cleaning data), 2 (exploratory data analysis), and 3 (preliminary model), all of them related to the capstone project from John´s Hopkins data science program.

The project is related to build a key that takes a (or more than 1)  word written by the user and (using ML prediction) suggest the next key.

Differently from suggested, instead of *tm* package, the *tidytext* approach will be used, mainly based on Julia Silge and David Robinson book “Text Mining with R: a Tidy approach”, delivered by Bookdown site (https://bookdown.org/)

## TASK 01: GETTING AND CLEANING THE DATA

### 1.1 Data download

The files were downloaded using a link available at Task 0 part of Coursera web site.

```{r DATA DOWLOAD, eval=F, message=F, warning=F}

fileLink <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

# Download raw data

download.file(url = fileLink,
              destfile = "dataset/rawData.zip")

# Extract the file

unzip("dataset/rawData.zip",
      exdir = "dataset")

```

### 1.2 Data import

The initial analysis will be focused on English language. The data from three sources (blogs, news and twitter) will be loaded at a list, to be transformed in a Corpus.

```{r DATA IMPORT, message=F, warning=F}

# Import data

rawData <- list(blog = readLines("dataset/final/en_US/en_US.blogs.txt", 
                                 warn = F,
                                 skipNul = T),
                news = readLines("dataset/final/en_US/en_US.news.txt",
                                 warn = F,
                                 skipNul = T),
                twitter = readLines(file("dataset/final/en_US/en_US.twitter.txt",
                                         open = "rb"),
                                    warn = F,
                                    skipNul = T)
)

closeAllConnections()

```

As blog and twitter text data are usually little messy (with a lot of misspellimg words, emoticons, contractions), the package *textclean* will be used to improve the readability of the text. In addition, *qdap* package will be used to check misspelling words.


```{r SAMPLE AND CLEAN}

plan(multiprocess, workers = availableCores() - 1)

# Sample and clean

rawSample <-
        rawData %>%
        future_map(function (x) {
          set.seed(123456)
          sample(x, 20000)}) 

# Check text problems for all sources.

textCheck <-
  rawSample %>%
  future_map(check_text)



        future_map(replace_contraction) %>%
        future_map(replace_date) %>%
        future_map(replace_email) %>%
        future_map(replace_emoticon) %>%
        future_map(replace_emoji) %>%
        future_map(replace_hash) %>%
        future_map(replace_incomplete) %>%
        future_map(replace_kern) %>%
        future_map(add_missing_endmark) %>%
        future_map(add_comma_space) %>%
        future_map(replace_non_ascii)
```

Raw data was sample to allow more velocity on posterior EDA and preliminary modeling and was tidified putting all data on a tibble.

In addition, the dataframe with profane words will be constructed, using all sources from *lexicon* package.

```{r PROFANE WORDS}

# Create profane words tibble

profaneWords <- 
        list(alvarez = toString(profanity_alvarez),
             arr_bad = toString(profanity_arr_bad),
             banned = toString(profanity_banned),
             racist = toString(profanity_racist),
             zac = toString(profanity_zac_anger)) %>%
        as_tibble() %>%
        gather(key = "lexicon") %>%
        unnest_tokens(word, value) %>%
        distinct(word, .keep_all = T)

```

### 1.3 Data cleaning

The cleaning process will have this steps:

* Tokenize sources,
* remove profane words,
* remove stop words,
* remove words that start with a number,
* remove URL´s,
* remove the words that are just punctuation.

```{r DATA CLEANING, message=F, warning=T}

# Tokenize data

unigram <-
        rawSample %>%
        unnest_tokens(word, value) %>%
        anti_join(profaneWords) %>%
        anti_join(stop_words) %>%
        filter(# one or more digit at beginning,
               !str_detect(word, "^\\d+"),
               # url 
               !str_detect(word, "(f|ht)tp(s?)://(.*)[.][a-z]+"),
               # one or more white space character
               !str_detect(word, "@[^\\s]+"),
               # detect words with 3 or more repetad characters
               !str_detect(word, "([[:alpha:]])\\1{3,}"),
               # Punctuation
               !stri_detect_regex(word,"[[:punct:]]|\\(.*\\)")) %>%
        count(word, sort = T) %>%
        mutate(total = sum(n),
               tf = n/sum(n))
```

The same process to tokenize and cleaning will be used to bi-grams, tri-grams and quad-grams.

```{r N-GRAMS, message=F}

# bi-grams

bigrams <-
  rawSample %>%
  unnest_tokens(word, value, token = "ngrams", n = 2) %>%
  count(source, word, sort = TRUE) %>%
  separate(word, c("word1", "word2"), sep = " ", remove = F) %>%
  filter(!word1 %in% profaneWords$word,
         !word2 %in% profaneWords$word,
         !word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !str_detect(word1, "^\\d+"),
         !str_detect(word1, "(f|ht)tp(s?)://(.*)[.][a-z]+"),
         !str_detect(word1, "â"),
         !str_detect(word1, "@[^\\s]+"),
         !str_detect(word1, "([[:alpha:]])\\1{3,}"),
         !stri_detect_regex(word1,"[0-9]+|[[:punct:]]|\\(.*\\)"),
         !str_detect(word2, "^\\d+"),
         !str_detect(word2, "â"),
         !str_detect(word2, "(f|ht)tp(s?)://(.*)[.][a-z]+"),
         !str_detect(word2, "@[^\\s]+"),
         !str_detect(word2, "([[:alpha:]])\\1{3,}"),
         !stri_detect_regex(word2,"[0-9]+|[[:punct:]]|\\(.*\\)")) %>%
  mutate(total = sum(n),
         tf = n/sum(n))

# tri-gram

trigrams <-
  rawSample %>%
  unnest_tokens(word, value, token = "ngrams", n = 3) %>%
  count(word, sort = TRUE) %>%
  separate(word, c("word1", "word2", "word3"), sep = " ", remove = F) %>%
  filter(!word1 %in% profaneWords$word,
         !word2 %in% profaneWords$word,
         !word3 %in% profaneWords$word,
         !word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word,
         !str_detect(word1, "^\\d+"),
         !str_detect(word1, "(f|ht)tp(s?)://(.*)[.][a-z]+"),
         !str_detect(word1, "@[^\\s]+"),
         !str_detect(word1, "â"),
         !str_detect(word1, "([[:alpha:]])\\1{3,}"),
         !stri_detect_regex(word1,"[0-9]+|[[:punct:]]|\\(.*\\)"),
         !str_detect(word2, "^\\d+"),
         !str_detect(word2, "â"),
         !str_detect(word2, "(f|ht)tp(s?)://(.*)[.][a-z]+"),
         !str_detect(word2, "@[^\\s]+"),
         !str_detect(word2, "([[:alpha:]])\\1{3,}"),
         !stri_detect_regex(word2,"[0-9]+|[[:punct:]]|\\(.*\\)"),
         !str_detect(word3, "^\\d+"),
         !str_detect(word3, "â"),
         !str_detect(word3, "(f|ht)tp(s?)://(.*)[.][a-z]+"),
         !str_detect(word3, "@[^\\s]+"),
         !str_detect(word3, "([[:alpha:]])\\1{3,}"),
         !stri_detect_regex(word3,"[0-9]+|[[:punct:]]|\\(.*\\)")) %>%
    mutate(total = sum(n),
         tf = n/sum(n))

# quad-gram

quadgrams <-
  rawSample %>%
  unnest_tokens(word, value, token = "ngrams", n = 4) %>%
  count(word, sort = TRUE) %>%
  separate(word, 
           c("word1", "word2", "word3", "word4"), 
           sep = " ", 
           remove = F) %>%
  filter(!word1 %in% profaneWords$word,
         !word2 %in% profaneWords$word,
         !word3 %in% profaneWords$word,
         !word4 %in% profaneWords$word,
         !word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word,
         !word4 %in% stop_words$word,
         !str_detect(word1, "^\\d+"),
         !str_detect(word1, "(f|ht)tp(s?)://(.*)[.][a-z]+"),
         !str_detect(word1, "@[^\\s]+"),
         !str_detect(word1, "â"),
         !str_detect(word1, "([[:alpha:]])\\1{3,}"),
         !stri_detect_regex(word1,"[0-9]+|[[:punct:]]|\\(.*\\)"),
         !str_detect(word2, "^\\d+"),
         !str_detect(word2, "(f|ht)tp(s?)://(.*)[.][a-z]+"),
         !str_detect(word2, "@[^\\s]+"),
         !str_detect(word2, "â"),
         !str_detect(word2, "([[:alpha:]])\\1{3,}"),
         !stri_detect_regex(word2,"[0-9]+|[[:punct:]]|\\(.*\\)"),
         !str_detect(word3, "^\\d+"),
         !str_detect(word3, "(f|ht)tp(s?)://(.*)[.][a-z]+"),
         !str_detect(word3, "@[^\\s]+"),
         !str_detect(word3, "â"),
         !str_detect(word3, "([[:alpha:]])\\1{3,}"),
         !stri_detect_regex(word3,"[0-9]+|[[:punct:]]|\\(.*\\)"),
         !str_detect(word4, "^\\d+"),
         !str_detect(word4, "(f|ht)tp(s?)://(.*)[.][a-z]+"),
         !str_detect(word4, "@[^\\s]+"),
         !str_detect(word4, "â"),
         !str_detect(word4, "([[:alpha:]])\\1{3,}"),
         !stri_detect_regex(word4,"[0-9]+|[[:punct:]]|\\(.*\\)")) %>%
      mutate(total = sum(n),
         tf = n/sum(n))

```

## TASK 02: EXPLORATORY DATA ANALYSIS

### 2.1 Ploting n-grams frequency

#### Uni-grams

```{r UNIGRAMS PLOT}

unigram %>%
  arrange(desc(n)) %>%
  group_by(source) %>%
  top_n(15) %>%
  mutate(word = factor(word, levels = rev(unique(word))))
  ungroup() %>%
  ggplot(aes(word, n, fill = source)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, 
       y = "Word count",
       man = "Unigram frequency") + 
  facet_wrap(~ source, ncol = 3, scales = "free") + 
  coord_flip()

```



## TASK 03: PRELIMINAR MODELING